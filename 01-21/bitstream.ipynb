{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the text files as numpy arrays. Since the data consists of integers, convert to int\n",
    "train_data = np.loadtxt('train.txt')\n",
    "test_data = np.loadtxt('test.txt')\n",
    "test_data = test_data.astype(int)\n",
    "train_data = train_data.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1]\n",
      "[1 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1\n",
      " 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0\n",
      " 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#see if it worked\n",
    "print(test_data)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is one way to define accuracy\n",
    "def entrywise_accuracy(guess, answer):\n",
    "    assert len(guess) == len(answer)\n",
    "    count = 0\n",
    "    for x, xt in zip(guess, answer):\n",
    "        if x == xt:\n",
    "            count += 1\n",
    "    return count / len(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7 18]\n"
     ]
    }
   ],
   "source": [
    "counts = np.bincount(test_data)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is another way to define accuracy, by the percentage of total ones \n",
    "def total_ones_accuracy(guess,answer):\n",
    "    assert len(guess) == len(answer)\n",
    "    countsguess = np.bincount(guess)\n",
    "    countsanswer = np.bincount(answer)\n",
    "    assert countsanswer[1] != 0\n",
    "    score = 1-np.abs(countsguess[1]-countsanswer[1])/countsanswer[1]\n",
    "    return float(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our metric make sense, they should give a value of 1 when the same data is fed in as the guess and the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entrywise_accuracy(test_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_ones_accuracy(test_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict1(train_data, N):\n",
    "    guess = train_data[len(train_data)-N:]\n",
    "    return guess\n",
    "def all_ones(N):\n",
    "    guess = [1 for _ in range(N)]\n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6111111111111112"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_ones_accuracy(all_ones(25),test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some potentially useful code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def predict2(train_data, N):\n",
    "    guess = []\n",
    "    for _ in range(N):\n",
    "        guess.append(random.choice(train_data))\n",
    "    return guess\n",
    "\n",
    "def predict3(train_data, N):\n",
    "    guess = [random.randint(0, 1) for _ in range(N)]\n",
    "    return guess\n",
    "\n",
    "def all_zeros(train_data, N):\n",
    "    guess = [0 for _ in range(N)]\n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_prediction(func):\n",
    "    guess = func(train_data, len(test_data))\n",
    "    acc = entrywise_accuracy(guess, test_data)\n",
    "    print(func.__name__, '\\t', acc)\n",
    "    \n",
    "for func in all_predicts:\n",
    "    score_prediction(func)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
